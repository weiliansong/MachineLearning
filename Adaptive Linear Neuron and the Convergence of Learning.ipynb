{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be looking at another type of single-layer neural network called Adaptive Linear Neuron, or Adaline for short. Adaline is different from the perceptron we build before because instead of using a step function to update the weights, we use a linear activation function instead. From the book I can tell that this is going to be an important concept to grasp, as it is the gateway to a lot of other more advanced algorithms that supports more than just binary classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For supervised learning, an <i>objective function</i> is extremely important as it is the function that we want the program to optimize during training, and it is usually a <i>cost function</i> that we want to minimize. Here is the cost function $J(w)$. $$J(w)=\\frac{1}{2}\\sum_{i}(y^{i}-\\phi(z^{i}))^{2}$$The term $\\frac{1}{2}$ will be explained later on, and the main advantage of this function over the step function is that it is differentiable, and using <i>gradient descent</i>, we can easily find ther lowest point of the graph, as it is quadratic and naturally convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update our weights, we would first take the partial derivative of our $J(w)$ function with respect to each weight $w_j$:$$\\frac{\\delta J}{\\delta w_j}=-\\sum_i(y^i-\\phi(z^i))*x^i_j$$To be quite honest, this is the place where I am a bit lost, as there are too many concepts for me to grasp at once. I know that we are minimizing our cost function, and somewhere along the line we are doing gradient descent to find the lowest point on graph. How do they relate to each other? Is it that the gradient descent determines the weights $w_j$ for each correspong $x_j$, and then those are used to minimize the function?\n",
    "\n",
    "So without fully grasping the concept of an Adaline neuron, I still wrote out the code for one, and here it is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdalineGD(object):\n",
    "    \"\"\"ADAptive LInear NEuron classifier.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Passes over the training dataset.\n",
    "        \n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting\n",
    "    errors_ : list\n",
    "        Numbers of misclassifications in every epoch\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            Training vectors,\n",
    "            where n_samples is the number of samples and\n",
    "            n_features is the number of features\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \n",
    "        \"\"\"\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
